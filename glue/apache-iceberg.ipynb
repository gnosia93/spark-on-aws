{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c676441e",
   "metadata": {},
   "source": [
    "AWSGlueServiceRoleJupyter 생성시 아래의 정책을 연결한다.. \n",
    "\n",
    "1.AmazonS3FullAccess <br>\n",
    "2.AWSGlueServiceRole <br>\n",
    "3.AWSGlueServiceRoleJupyter-Dynamo (Inline) <br>\n",
    "  {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"AllAPIActionsOnBooks\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"dynamodb:*\",\n",
    "            \"Resource\": \"arn:aws:dynamodb:ap-northeast-2:499514681453:table/iceberg-lock\"\n",
    "        }\n",
    "    ]\n",
    "}  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fb57ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are already connected to a glueetl session 16c5c365-b73e-45ee-8ba7-d4d283c7e6bf.\n",
      "\n",
      "No change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Glue version to: 3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are already connected to a glueetl session 16c5c365-b73e-45ee-8ba7-d4d283c7e6bf.\n",
      "\n",
      "No change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous number of workers: 2\n",
      "Setting new number of workers to: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are already connected to a glueetl session 16c5c365-b73e-45ee-8ba7-d4d283c7e6bf.\n",
      "\n",
      "No change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous worker type: G.2X\n",
      "Setting new worker type to: G.2X\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are already connected to a glueetl session 16c5c365-b73e-45ee-8ba7-d4d283c7e6bf.\n",
      "\n",
      "No change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current idle_timeout is 6000 minutes.\n",
      "idle_timeout has been set to 6000 minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are already connected to a glueetl session 16c5c365-b73e-45ee-8ba7-d4d283c7e6bf.\n",
      "\n",
      "No change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous region: ap-northeast-2\n",
      "Setting new region to: ap-northeast-2\n",
      "Reauthenticating Glue client with new region: ap-northeast-2\n",
      "IAM role has been set to arn:aws:iam::499514681453:role/AWSGlueServiceRoleJupyter. Reauthenticating.\n",
      "Authenticating with profile=default\n",
      "glue_role_arn defined by user: arn:aws:iam::499514681453:role/AWSGlueServiceRoleJupyter\n",
      "Authentication done.\n",
      "Region is set to: ap-northeast-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are already connected to a glueetl session 16c5c365-b73e-45ee-8ba7-d4d283c7e6bf.\n",
      "\n",
      "No change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iam_role is arn:aws:iam::499514681453:role/AWSGlueServiceRoleJupyter\n",
      "iam_role has been set to arn:aws:iam::499514681453:role/AWSGlueServiceRoleJupyter.\n"
     ]
    }
   ],
   "source": [
    "%glue_version 3.0\n",
    "%number_of_workers 2\n",
    "%worker_type G.2X\n",
    "%idle_timeout 6000\n",
    "%region ap-northeast-2\n",
    "%iam_role arn:aws:iam::499514681453:role/AWSGlueServiceRoleJupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81875c40",
   "metadata": {},
   "source": [
    "https://docs.aws.amazon.com/ko_kr/glue/latest/dg/aws-glue-programming-etl-format-iceberg.html <br>\n",
    "IceBerg 테이블을 생성하기 위해서는 GLUE JOB 실행 파라미터로 --datalake-formats = iceberg 값을 줘야한다. \n",
    "GLUE 콘솔에서 JOB을 등록할때 실행 파라미터 값을 설정하거나, CLI 커맨드를 이용하여 JOB 을 등록할 떄 값을 설정해야 한다.\n",
    "이값을 설정하지 않으면 코드 실행시 glue_catalog 플러그인을 찾을 수 없다는 오류가 발생한다. \n",
    "이에 반해, 이 노트북에서 처럼 glue interactive 모드를 사용하는 경우는 아래와 같이 %%configure 매직을 활용하여 datalake format 이 iceberg 로 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28b4850e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following configurations have been updated: {'datalake-formats': 'iceberg'}\n"
     ]
    }
   ],
   "source": [
    "%%configure \n",
    "{ \"datalake-formats\":\"iceberg\" }                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78ba4032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticating with profile=default\n",
      "glue_role_arn defined by user: arn:aws:iam::499514681453:role/AWSGlueServiceRoleJupyter\n",
      "Trying to create a Glue session for the kernel.\n",
      "Worker Type: G.2X\n",
      "Number of Workers: 2\n",
      "Session ID: 16c5c365-b73e-45ee-8ba7-d4d283c7e6bf\n",
      "Job Type: glueetl\n",
      "Applying the following default arguments:\n",
      "--glue_kernel_version 0.37.2\n",
      "--enable-glue-datacatalog true\n",
      "--datalake-formats iceberg\n",
      "Waiting for session 16c5c365-b73e-45ee-8ba7-d4d283c7e6bf to get into ready status...\n",
      "Session 16c5c365-b73e-45ee-8ba7-d4d283c7e6bf has been created.\n",
      "+--------+--------------------+-----------+\n",
      "|database|           tableName|isTemporary|\n",
      "+--------+--------------------+-----------+\n",
      "|  sample|             iceberg|      false|\n",
      "|  sample|             tbl-csv|      false|\n",
      "|  sample|       tbl-flight-p5|      false|\n",
      "|  sample|     tbl-flight-part|      false|\n",
      "|  sample|tbl-flight-part-r...|      false|\n",
      "|  sample|         tbl-flight4|      false|\n",
      "+--------+--------------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "show tables in `sample`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f21c84",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.SparkSession.html\n",
    "glue spark 에서 iceberg 테이블을 사용하기 위해서는 아래와 같이 config 를 설정해야 한다. 코드상에 spark 세션의 .config 함수를 활용할 수도 있고,\n",
    "glue job 의 파리미터로 설정할 수도 있다. 지금처럼 인터랙티브 모드로 개발하는 경우에는 spark session 에 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4bcbec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=jes appName=GlueReplApp>\n"
     ]
    }
   ],
   "source": [
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\"\"\"\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "SparkSession = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "\"\"\"\n",
    "\n",
    "# Iceberg configuration\n",
    "warehouse_path = 's3://glue-seoul-20230405/iceberg'\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"iceberg DDL/DML\") \\\n",
    "    .config('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions') \\\n",
    "    .config('spark.sql.catalog.glue_catalog', 'org.apache.iceberg.spark.SparkCatalog') \\\n",
    "    .config('spark.sql.catalog.glue_catalog.warehouse', warehouse_path) \\\n",
    "    .config('spark.sql.catalog.glue_catalog.catalog-impl', 'org.apache.iceberg.aws.glue.GlueCatalog') \\\n",
    "    .config('spark.sql.catalog.glue_catalog.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO') \\\n",
    "    .config('spark.sql.catalog.glue_catalog.lock-impl', 'org.apache.iceberg.aws.glue.DynamoLockManager') \\\n",
    "    .config('spark.sql.catalog.glue_catalog.lock.table', 'iceberg-lock') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "glueContext = GlueContext(sc)\n",
    "#job = Job(glueContext)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67437939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|year|quarter|month|\n",
      "+----+-------+-----+\n",
      "|2016|      4|   10|\n",
      "|2016|      4|   10|\n",
      "|2016|      4|   10|\n",
      "+----+-------+-----+\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "select year, quarter, month from sample.`tbl-flight4` limit 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6bb0bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnalysisException: Table sample.iceberg already exists\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "create table `glue_catalog`.sample.`iceberg`\n",
    "using iceberg\n",
    "select * from sample.`tbl-flight4`\n",
    "\"\"\"\n",
    "spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a5894eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+\n",
      "|database|           tableName|isTemporary|\n",
      "+--------+--------------------+-----------+\n",
      "|  sample|             iceberg|      false|\n",
      "|  sample|             tbl-csv|      false|\n",
      "|  sample|       tbl-flight-p5|      false|\n",
      "|  sample|     tbl-flight-part|      false|\n",
      "|  sample|tbl-flight-part-r...|      false|\n",
      "|  sample|         tbl-flight4|      false|\n",
      "+--------+--------------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "show tables in `sample`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c66f0eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-------+\n",
      "|       col_name|data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|           year|   bigint|       |\n",
      "|        quarter|   bigint|       |\n",
      "|          month|   bigint|       |\n",
      "|   day_of_month|   bigint|       |\n",
      "|    day_of_week|   bigint|       |\n",
      "|        fl_date|   string|       |\n",
      "| unique_carrier|   string|       |\n",
      "|     airline_id|   bigint|       |\n",
      "|        carrier|   string|       |\n",
      "|       tail_num|   string|       |\n",
      "|         fl_num|   bigint|       |\n",
      "|   crs_dep_time|   bigint|       |\n",
      "|       dep_time|   bigint|       |\n",
      "|      dep_delay|   bigint|       |\n",
      "|  dep_delay_new|   bigint|       |\n",
      "|      dep_del15|   bigint|       |\n",
      "|dep_delay_group|   bigint|       |\n",
      "|   dep_time_blk|   string|       |\n",
      "|       taxi_out|   bigint|       |\n",
      "|     wheels_off|   bigint|       |\n",
      "+---------------+---------+-------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "describe table `glue_catalog`.sample.`iceberg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c49ab5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 6288052|\n",
      "+--------+\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "select count(1) from `glue_catalog`.sample.`iceberg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60a87a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|month|   cnt|\n",
      "+-----+------+\n",
      "|   12|460949|\n",
      "|    2|423889|\n",
      "|    9|454878|\n",
      "|   11|970536|\n",
      "|    7|502457|\n",
      "|    3|479122|\n",
      "|    8|498347|\n",
      "|    5|479358|\n",
      "|    4|461630|\n",
      "|   10|623422|\n",
      "|    1|445827|\n",
      "|    6|487637|\n",
      "+-----+------+\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "select month, count(1) as cnt from `glue_catalog`.sample.`iceberg` group by month;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "996a6333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "insert into `glue_catalog`.sample.`iceberg` select * from `glue_catalog`.sample.`iceberg` where month = 11;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90664b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 6288052|\n",
      "+--------+\n"
     ]
    }
   ],
   "source": [
    "countDf = spark.sql(\"select count(1) from `glue_catalog`.sample.`iceberg`\")\n",
    "countDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd3c4dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|airline_id|    cnt|\n",
      "+----------+-------+\n",
      "|     19393|1302080|\n",
      "|     19790|1176192|\n",
      "|     20304| 796656|\n",
      "|     19805| 769669|\n",
      "|     19977| 708422|\n",
      "|     20366| 640132|\n",
      "|     20409| 247554|\n",
      "|     20416| 184523|\n",
      "|     19930| 156300|\n",
      "|     20436| 126935|\n",
      "|     19690| 101243|\n",
      "|     21171|  78346|\n",
      "+----------+-------+\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "select airline_id, count(1) as cnt from `glue_catalog`.sample.`iceberg` \n",
    "group by airline_id\n",
    "order by cnt desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4697716b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n"
     ]
    }
   ],
   "source": [
    "%%sql \n",
    "delete from `glue_catalog`.sample.`iceberg`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90bd4f6",
   "metadata": {},
   "source": [
    "https://iceberg.apache.org/docs/latest/spark-writes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8b82a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "select count(1) from `glue_catalog`.sample.`iceberg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf8aaead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "insert into `glue_catalog`.sample.`iceberg`\n",
    "select * from sample.`tbl-flight4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71bcbeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 5248439|\n",
      "+--------+\n"
     ]
    }
   ],
   "source": [
    "%%sql \n",
    "select count(1) from `glue_catalog`.sample.`iceberg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "329ce733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Py4JJavaError: An error occurred while calling o69.sql.\n",
      ": java.lang.UnsupportedOperationException: UPDATE TABLE is not supported temporarily.\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies$BasicOperators$.apply(SparkStrategies.scala:716)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:480)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:485)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)\n",
      "\tat scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:937)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:937)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1425)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1425)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:480)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:486)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:67)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:426)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:192)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:163)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:104)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:117)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:192)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:163)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$writePlans$5(QueryExecution.scala:245)\n",
      "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$.append(QueryPlan.scala:487)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.writePlans(QueryExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toString(QueryExecution.scala:260)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:216)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:102)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3722)\n",
      "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\n",
      "\tat sun.reflect.GeneratedMethodAccessor104.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "update `glue_catalog`.sample.`iceberg`\n",
    "set\n",
    "  taxi_out = 0\n",
    "where month = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fccbe4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnalysisException: Cannot delete from table glue_catalog.sample.iceberg where [EqualTo(month,12)]\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "delete from `glue_catalog`.sample.`iceberg` where month = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c16965d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'3.1.1-amzn-0'\n"
     ]
    }
   ],
   "source": [
    "sc.version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1e9fb8",
   "metadata": {},
   "source": [
    "https://docs.aws.amazon.com/glue/latest/dg/release-notes.html  <br>\n",
    "glue 3.0 의 경우 iceberg 버전 0.13.1 을 지원함\n",
    "glue 4.0 의 경우 iceberg 버전 1.0.0 을 지원함.\n",
    "인터랙티브 노트북은 glue 3.0 까지는 지원하는 관계로, 이 노트북에서 사용하는 버전이 iceberg 0.13.1 인 관계로 DML이 제대로 지원되지 않은듯 .. ??!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3de004e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "ALTER TABLE `glue_catalog`.sample.`iceberg`\n",
    "ADD COLUMN `add-col` bigint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07afea99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-------+\n",
      "|           col_name|data_type|comment|\n",
      "+-------------------+---------+-------+\n",
      "|               year|   bigint|       |\n",
      "|            quarter|   bigint|       |\n",
      "|              month|   bigint|       |\n",
      "|       day_of_month|   bigint|       |\n",
      "|        day_of_week|   bigint|       |\n",
      "|            fl_date|   string|       |\n",
      "|     unique_carrier|   string|       |\n",
      "|         airline_id|   bigint|       |\n",
      "|            carrier|   string|       |\n",
      "|           tail_num|   string|       |\n",
      "|             fl_num|   bigint|       |\n",
      "|       crs_dep_time|   bigint|       |\n",
      "|           dep_time|   bigint|       |\n",
      "|          dep_delay|   bigint|       |\n",
      "|      dep_delay_new|   bigint|       |\n",
      "|          dep_del15|   bigint|       |\n",
      "|    dep_delay_group|   bigint|       |\n",
      "|       dep_time_blk|   string|       |\n",
      "|           taxi_out|   bigint|       |\n",
      "|         wheels_off|   bigint|       |\n",
      "|          wheels_on|   bigint|       |\n",
      "|            taxi_in|   bigint|       |\n",
      "|       crs_arr_time|   bigint|       |\n",
      "|           arr_time|   bigint|       |\n",
      "|          arr_delay|   bigint|       |\n",
      "|      arr_delay_new|   bigint|       |\n",
      "|          arr_del15|   bigint|       |\n",
      "|    arr_delay_group|   bigint|       |\n",
      "|       arr_time_blk|   string|       |\n",
      "|          cancelled|   bigint|       |\n",
      "|  cancellation_code|   string|       |\n",
      "|           diverted|   bigint|       |\n",
      "|   crs_elapsed_time|   bigint|       |\n",
      "|actual_elapsed_time|   bigint|       |\n",
      "|           air_time|   bigint|       |\n",
      "|            flights|   bigint|       |\n",
      "|           distance|   bigint|       |\n",
      "|     distance_group|   bigint|       |\n",
      "|                mon|   string|       |\n",
      "|      carrier_delay|   bigint|       |\n",
      "|      weather_delay|   bigint|       |\n",
      "|          nas_delay|   bigint|       |\n",
      "|     security_delay|   bigint|       |\n",
      "|late_aircraft_delay|   bigint|       |\n",
      "|     first_dep_time|   bigint|       |\n",
      "|    total_add_gtime|   bigint|       |\n",
      "|  longest_add_gtime|   bigint|       |\n",
      "|            add-col|   bigint|       |\n",
      "|                   |         |       |\n",
      "|     # Partitioning|         |       |\n",
      "|    Not partitioned|         |       |\n",
      "+-------------------+---------+-------+\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe `glue_catalog`.sample.`iceberg`\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5eefbc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "ALTER TABLE `glue_catalog`.sample.`iceberg` DROP COLUMN longest_add_gtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "39f07e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-------+\n",
      "|           col_name|data_type|comment|\n",
      "+-------------------+---------+-------+\n",
      "|               year|   bigint|       |\n",
      "|            quarter|   bigint|       |\n",
      "|              month|   bigint|       |\n",
      "|       day_of_month|   bigint|       |\n",
      "|        day_of_week|   bigint|       |\n",
      "|            fl_date|   string|       |\n",
      "|     unique_carrier|   string|       |\n",
      "|         airline_id|   bigint|       |\n",
      "|            carrier|   string|       |\n",
      "|           tail_num|   string|       |\n",
      "|             fl_num|   bigint|       |\n",
      "|       crs_dep_time|   bigint|       |\n",
      "|           dep_time|   bigint|       |\n",
      "|          dep_delay|   bigint|       |\n",
      "|      dep_delay_new|   bigint|       |\n",
      "|          dep_del15|   bigint|       |\n",
      "|    dep_delay_group|   bigint|       |\n",
      "|       dep_time_blk|   string|       |\n",
      "|           taxi_out|   bigint|       |\n",
      "|         wheels_off|   bigint|       |\n",
      "|          wheels_on|   bigint|       |\n",
      "|            taxi_in|   bigint|       |\n",
      "|       crs_arr_time|   bigint|       |\n",
      "|           arr_time|   bigint|       |\n",
      "|          arr_delay|   bigint|       |\n",
      "|      arr_delay_new|   bigint|       |\n",
      "|          arr_del15|   bigint|       |\n",
      "|    arr_delay_group|   bigint|       |\n",
      "|       arr_time_blk|   string|       |\n",
      "|          cancelled|   bigint|       |\n",
      "|  cancellation_code|   string|       |\n",
      "|           diverted|   bigint|       |\n",
      "|   crs_elapsed_time|   bigint|       |\n",
      "|actual_elapsed_time|   bigint|       |\n",
      "|           air_time|   bigint|       |\n",
      "|            flights|   bigint|       |\n",
      "|           distance|   bigint|       |\n",
      "|     distance_group|   bigint|       |\n",
      "|                mon|   string|       |\n",
      "|      carrier_delay|   bigint|       |\n",
      "|      weather_delay|   bigint|       |\n",
      "|          nas_delay|   bigint|       |\n",
      "|     security_delay|   bigint|       |\n",
      "|late_aircraft_delay|   bigint|       |\n",
      "|     first_dep_time|   bigint|       |\n",
      "|    total_add_gtime|   bigint|       |\n",
      "|            add-col|   bigint|       |\n",
      "|                   |         |       |\n",
      "|     # Partitioning|         |       |\n",
      "|    Not partitioned|         |       |\n",
      "+-------------------+---------+-------+\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe `glue_catalog`.sample.`iceberg`\").show(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Glue_Session",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
